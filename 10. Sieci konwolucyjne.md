# Głębokie sieci neuronowe 1

Rozwiniemy i poprawimy wynik zadania z poprzednich zajęć.

## Zadanie 1

Na podstawie Twojego rozwiązania z poprzednich zajęć przygotuj jeszcze dwie wersje. Niech różnią się one tym, ze w jednej dodasz jedną warstwę konwolucyjną na początku a w drugiej dwie. Zmniejsz część w pełni połączoną (według swojego uznania). Proszę nie korzystać do rozwiązania z LLM-ów bo każdy otrzyma takie samo rozwiązanie, a lepiej, aby tak nie było. Pamiętaj że liczba filtrów powinna być większa niż 1. Flatten powinien pojawić się po warstwach konwolucyjnych (https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).

Pamiętaj, aby zmienić nazwy plików modeli (bo architektury będą różne).

## Zadanie 2

Przetestuj wszystkie wersje - uruchom dla takich samych parametrów liczby epok i rozmiaru minigrupy (batch size). Zanotuj sobie, jakie masz wyniki. Wskaż który model działa najlepiej.

## Zadanie 3

Zamień optymalzator z SGD na Adam. Uruchom wszystkie modele. Dopisz wyniki do swojego zestawienia i wskaż który model działa najlepiej.

## Zadanie domowe

Zastosuj augmentację danych w celu poprawy możliwości Twoich modeli. Omówienie możliwości augmentacji dotyczace obrazów jest tu: https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html . Przetestuj na obrazkach obuwia z internetu. Warto losowo odwrócić kolory (aby tło było czasami białe), wtedy jest szansa, że klasyfikator poradzi sobie lepiej z nowymi obrazami.

## Zadanie dodatkowe

Dla ambitnych, ale może być ciekawe:

Zapoznaj się z architekturą autoenkodera. Spróbuj zrobić przykład na tej podstawie podobny do https://www.tensorflow.org/tutorials/generative/autoencoder ale w PyTorch.
